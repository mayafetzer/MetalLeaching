{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrzT4RyjCtJ1WjRIZS/w5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayafetzer/MetalLeaching/blob/main/MetalExtractionMLModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMpRZxP8pCoH",
        "outputId": "5130e960-36ab-4345-f603-7b1647f7fcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Full pipeline: training, tuning, CV plots, SHAP/importance, Streamlit app, export predictions\n",
        "!pip install --quiet pandas numpy scikit-learn matplotlib seaborn openpyxl shap streamlit xlrd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# 1) Upload and prepare data\n",
        "# -------------------------\n",
        "print(\"Upload your Excel file (ReducingAgentData.xlsx):\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "df_raw = pd.read_excel(file_path, header=None)\n",
        "\n",
        "# Fix headers (drop first grouped row and promote second row)\n",
        "df = df_raw.drop(index=0).reset_index(drop=True)\n",
        "raw_header = df.iloc[0].astype(str)\n",
        "clean_header = (\n",
        "    raw_header.str.replace(r\"^\\s*0\\s*\", \"\", regex=True)\n",
        "              .str.replace(\"\\n\", \" \", regex=False)\n",
        "              .str.replace(\"\\r\", \" \", regex=False)\n",
        "              .str.strip()\n",
        "              .str.replace(\"  \", \" \")\n",
        ")\n",
        "df.columns = clean_header\n",
        "df = df.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# Define columns (as you specified)\n",
        "categorical_cols = [\"Leaching agent\", \"Type of reducing agent\"]\n",
        "numeric_cols = [\n",
        "    \"Li in feed %\",\n",
        "    \"Co in feed %\",\n",
        "    \"Mn in feed %\",\n",
        "    \"Ni in feed %\",\n",
        "    \"Concentration, M\",\n",
        "    \"Concentration %\",\n",
        "    \"Time,min\",\n",
        "    \"Temperature, C\"\n",
        "]\n",
        "target_cols = [\"Li\", \"Co\", \"Mn\", \"Ni\"]\n",
        "\n",
        "# Ensure numeric conversion\n",
        "for c in numeric_cols + target_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "# Drop rows missing any target\n",
        "df = df.dropna(subset=target_cols).reset_index(drop=True)\n",
        "print(\"Dataset after dropping rows without all targets:\", df.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 2) EDA heatmap\n",
        "# -------------------------\n",
        "plt.figure(figsize=(12,8))\n",
        "corr = df[numeric_cols + target_cols].corr()\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"heatmap_numeric_targets.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 3) Preprocessors\n",
        "# -------------------------\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor_withcat = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_cols),\n",
        "    (\"cat\", categorical_pipeline, categorical_cols)\n",
        "])\n",
        "\n",
        "preprocessor_nocat = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_cols)\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# 4) Model candidates & param grids (for RandomizedSearch)\n",
        "# -------------------------\n",
        "candidates = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(),\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "param_distributions = {\n",
        "    \"LinearRegression\": {\n",
        "        # very few params to tune for quickness\n",
        "        # LinearRegression has no many hyperparams; we'll vary fit_intercept\n",
        "        \"model__fit_intercept\": [True, False]\n",
        "    },\n",
        "    \"Ridge\": {\n",
        "        \"model__alpha\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        \"model__solver\": [\"auto\"]\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"model__n_estimators\": [100, 200, 400],\n",
        "        \"model__max_depth\": [3, 5, 10, None],\n",
        "        \"model__min_samples_split\": [2, 5, 10],\n",
        "        \"model__max_features\": [\"auto\", \"sqrt\"]\n",
        "    },\n",
        "    \"GradientBoosting\": {\n",
        "        \"model__n_estimators\": [100, 200, 400],\n",
        "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
        "        \"model__max_depth\": [2, 3, 4]\n",
        "    }\n",
        "}\n",
        "\n",
        "# RandomizedSearch settings\n",
        "n_iter_search = 20\n",
        "cv_folds = 5\n",
        "random_state = 42\n",
        "\n",
        "# Create output dirs\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"cv_plots\", exist_ok=True)\n",
        "os.makedirs(\"importance_plots\", exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Helper functions\n",
        "# -------------------------\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def tune_and_evaluate(pipe, param_dist, X_train, y_train):\n",
        "    # Randomized search\n",
        "    rnd = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=n_iter_search,\n",
        "                             scoring=\"r2\", cv=cv_folds, n_jobs=-1, random_state=random_state, verbose=0)\n",
        "    rnd.fit(X_train, y_train)\n",
        "    return rnd\n",
        "\n",
        "def plot_cv_r2(model, X, y, title, outpath):\n",
        "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    scores = cross_val_score(model, X, y, scoring=\"r2\", cv=kf, n_jobs=-1)\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1, cv_folds+1), scores, marker='o', linestyle='-')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"R²\")\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(outpath, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return scores\n",
        "\n",
        "def compute_and_plot_importance(model, preprocessor, X, y, outpath, top_n=20, model_name=\"model\"):\n",
        "    \"\"\"\n",
        "    If tree-based model -> SHAP TreeExplainer (fast).\n",
        "    Else -> permutation importance.\n",
        "    \"\"\"\n",
        "    # Get preprocessed X (2d array) and feature names\n",
        "    X_proc = preprocessor.transform(X)\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "    except:\n",
        "        # fallback\n",
        "        # numeric names + ohe feature names\n",
        "        num_names = numeric_cols\n",
        "        cat_transformer = preprocessor.named_transformers_['cat'] if 'cat' in preprocessor.named_transformers_ else None\n",
        "        cat_names = []\n",
        "        if cat_transformer is not None:\n",
        "            try:\n",
        "                ohe = cat_transformer.named_steps['onehot']\n",
        "                cat_names = list(ohe.get_feature_names_out(categorical_cols))\n",
        "            except:\n",
        "                cat_names = []\n",
        "        feature_names = list(num_names) + list(cat_names)\n",
        "\n",
        "    # If underlying estimator is tree-based, try SHAP TreeExplainer\n",
        "    base_est = model.named_steps['model']\n",
        "    try:\n",
        "        if isinstance(base_est, (RandomForestRegressor, GradientBoostingRegressor)):\n",
        "            explainer = shap.TreeExplainer(base_est)\n",
        "            shap_values = explainer.shap_values(X_proc)\n",
        "            # shap_values will be 2D array (n_samples, n_features)\n",
        "            shap_abs = np.abs(shap_values).mean(axis=0)\n",
        "            imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": shap_abs})\n",
        "            imp_df = imp_df.sort_values(\"importance\", ascending=False).head(top_n)\n",
        "            # Plot\n",
        "            plt.figure(figsize=(8, max(4, 0.25*len(imp_df))))\n",
        "            sns.barplot(x=\"importance\", y=\"feature\", data=imp_df)\n",
        "            plt.title(f\"SHAP mean(|value|) feature importance ({model_name})\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(outpath, dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            return imp_df\n",
        "    except Exception as e:\n",
        "        print(\"SHAP failed or not applicable:\", e)\n",
        "\n",
        "    # Fallback: permutation importance (slower)\n",
        "    print(\"Using permutation importance (fallback). This may take some time.\")\n",
        "    r = permutation_importance(model, X, y, n_repeats=10, random_state=random_state, n_jobs=-1)\n",
        "    importances = r.importances_mean\n",
        "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "    imp_df = imp_df.sort_values(\"importance\", ascending=False).head(top_n)\n",
        "    plt.figure(figsize=(8, max(4, 0.25*len(imp_df))))\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=imp_df)\n",
        "    plt.title(f\"Permutation importance ({model_name})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return imp_df\n",
        "\n",
        "# -------------------------\n",
        "# 6) Loop over targets and both pipelines\n",
        "# -------------------------\n",
        "summary_rows = []\n",
        "\n",
        "for use_cat, preprocessor, label in [\n",
        "    (True, preprocessor_withcat, \"withcat\"),\n",
        "    (False, preprocessor_nocat, \"nocat\")\n",
        "]:\n",
        "    print(\"\\n\\n==============================\")\n",
        "    print(\"Pipeline:\", label)\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # Select X for pipeline\n",
        "    X_all = df[numeric_cols + (categorical_cols if use_cat else [])]\n",
        "\n",
        "    for target in target_cols:\n",
        "        print(\"\\n----------------------------------------------\")\n",
        "        print(f\"TARGET: {target}  |  PIPELINE: {label}\")\n",
        "        print(\"----------------------------------------------\")\n",
        "\n",
        "        y_all = df[target]\n",
        "\n",
        "        # train/test split (for tuning)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=random_state)\n",
        "\n",
        "        # Evaluate each base candidate and record simple baseline\n",
        "        base_results = []\n",
        "        for name, estimator in candidates.items():\n",
        "            pipe = Pipeline([(\"preprocessing\", preprocessor), (\"model\", estimator)])\n",
        "            pipe.fit(X_train, y_train)\n",
        "            preds = pipe.predict(X_test)\n",
        "            rmse = mean_squared_error(y_test, preds)\n",
        "            r2 = r2_score(y_test, preds)\n",
        "            base_results.append((name, rmse, r2))\n",
        "            print(f\"[BASE] {name:18s} RMSE={rmse:.4f}  R2={r2:.4f}\")\n",
        "\n",
        "        # pick winner (based on RMSE)\n",
        "        base_results.sort(key=lambda x: x[1])\n",
        "        best_base_name = base_results[0][0]\n",
        "        print(f\"\\nInitial best model (by RMSE) for {target} ({label}): {best_base_name}\")\n",
        "\n",
        "        # Prepare param dist keyed by best_base_name\n",
        "        param_dist = param_distributions.get(best_base_name, {})\n",
        "        pipe_for_tune = Pipeline([(\"preprocessing\", preprocessor), (\"model\", candidates[best_base_name])])\n",
        "\n",
        "        # Attach parameter prefixes to param_dist keys are already prefixed with model__\n",
        "        # Run RandomizedSearchCV\n",
        "        if len(param_dist) > 0:\n",
        "            print(\"Starting RandomizedSearchCV (this may take a while)...\")\n",
        "            rnd = RandomizedSearchCV(pipe_for_tune, param_distributions=param_dist, n_iter=n_iter_search,\n",
        "                                     scoring=\"r2\", cv=cv_folds, n_jobs=-1, random_state=random_state, verbose=1)\n",
        "            rnd.fit(X_train, y_train)\n",
        "            tuned = rnd.best_estimator_\n",
        "            print(\"Tuning complete. Best params:\", rnd.best_params_, \"Best CV R2:\", rnd.best_score_)\n",
        "        else:\n",
        "            print(\"No param grid for this model; skipping tuning.\")\n",
        "            tuned = pipe_for_tune\n",
        "            tuned.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate tuned model on held-out test\n",
        "        preds_test = tuned.predict(X_test)\n",
        "        test_rmse = mean_squared_error(y_test, preds_test)\n",
        "        test_r2 = r2_score(y_test, preds_test)\n",
        "        print(f\"Test RMSE (tuned): {test_rmse:.4f}  Test R² (tuned): {test_r2:.4f}\")\n",
        "\n",
        "        # Save tuned model\n",
        "        model_fname = f\"models/best_tuned_{label}_{target}.pkl\"\n",
        "        with open(model_fname, \"wb\") as f:\n",
        "            pickle.dump(tuned, f)\n",
        "        print(\"Saved tuned model:\", model_fname)\n",
        "\n",
        "        # 5-fold CV R2 plot (using tuned model)\n",
        "        cv_plot_path = f\"cv_plots/cv_r2_{label}_{target}.png\"\n",
        "        try:\n",
        "            cv_scores = plot_cv_r2(tuned, X_all, y_all, f\"5-Fold CV R² ({label} - {target})\", cv_plot_path)\n",
        "            print(\"CV R² scores:\", cv_scores, \"mean:\", np.mean(cv_scores))\n",
        "        except Exception as e:\n",
        "            print(\"Error computing cross-val scores:\", e)\n",
        "            cv_scores = []\n",
        "\n",
        "        # Feature importance / SHAP plot\n",
        "        imp_plot_path = f\"importance_plots/importance_{label}_{target}.png\"\n",
        "        try:\n",
        "            imp_df = compute_and_plot_importance(tuned, preprocessor, X_all, y_all, imp_plot_path, top_n=25, model_name=f\"{label}_{target}\")\n",
        "            print(\"Saved importance plot:\", imp_plot_path)\n",
        "        except Exception as e:\n",
        "            print(\"Importance computation failed:\", e)\n",
        "\n",
        "        # Save predictions on full dataset using tuned model\n",
        "        preds_full = tuned.predict(X_all)\n",
        "        df[f\"pred_{label}_{target}\"] = preds_full\n",
        "\n",
        "        # Record summary\n",
        "        summary_rows.append({\n",
        "            \"pipeline\": label,\n",
        "            \"target\": target,\n",
        "            \"base_model\": best_base_name,\n",
        "            \"test_rmse\": test_rmse,\n",
        "            \"test_r2\": test_r2,\n",
        "            \"cv_mean_r2\": float(np.mean(cv_scores)) if len(cv_scores)>0 else np.nan,\n",
        "            \"model_file\": model_fname\n",
        "        })\n",
        "\n",
        "# -------------------------\n",
        "# 7) Save full predictions to Excel\n",
        "# -------------------------\n",
        "out_pred_file = \"predictions_full_dataset.xlsx\"\n",
        "df.to_excel(out_pred_file, index=False)\n",
        "print(\"\\nSaved full-dataset predictions to:\", out_pred_file)\n",
        "files.download(out_pred_file)\n",
        "\n",
        "# -------------------------\n",
        "# 8) Save summary CSV\n",
        "# -------------------------\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df.to_csv(\"tuning_summary.csv\", index=False)\n",
        "print(\"Saved tuning summary to tuning_summary.csv\")\n",
        "files.download(\"tuning_summary.csv\")\n",
        "\n",
        "# -------------------------\n",
        "# 9) Generate Streamlit app file (streamlit_app.py)\n",
        "# -------------------------\n",
        "streamlit_code = r'''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "st.title(\"Predict Li/Co/Mn/Ni - Tuned Models\")\n",
        "st.write(\"Upload a CSV with the input columns (same names as in training).\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
        "if uploaded:\n",
        "    df = pd.read_csv(uploaded)\n",
        "    st.write(\"Preview:\")\n",
        "    st.dataframe(df.head())\n",
        "\n",
        "    # Load models\n",
        "    models = {}\n",
        "    for label in [\"withcat\", \"nocat\"]:\n",
        "        for metal in [\"Li\",\"Co\",\"Mn\",\"Ni\"]:\n",
        "            fname = f\"models/best_tuned_{label}_{metal}.pkl\"\n",
        "            if os.path.exists(fname):\n",
        "                models[f\"{label}_{metal}\"] = pickle.load(open(fname,\"rb\"))\n",
        "\n",
        "    st.write(\"Loaded models:\", list(models.keys()))\n",
        "\n",
        "    if st.button(\"Run predictions\"):\n",
        "        # Decide pipeline type by presence of categorical columns\n",
        "        need_cat = \"Leaching agent\" in df.columns and \"Type of reducing agent\" in df.columns\n",
        "\n",
        "        results = df.copy()\n",
        "        for key, model in models.items():\n",
        "            # skip models that don't match input features (withcat vs nocat)\n",
        "            if (\"withcat\" in key and not need_cat) or (\"nocat\" in key and need_cat and False):\n",
        "                # only run nocat if we only have numeric inputs; run both otherwise\n",
        "                pass\n",
        "            try:\n",
        "                preds = model.predict(df)\n",
        "                results[f\"pred_{key}\"] = preds\n",
        "            except Exception as e:\n",
        "                st.write(\"Failed to predict with model\", key, \":\", e)\n",
        "\n",
        "        st.write(\"Predictions sample:\")\n",
        "        st.dataframe(results.head())\n",
        "\n",
        "        outname = \"predictions_from_streamlit.xlsx\"\n",
        "        results.to_excel(outname, index=False)\n",
        "        st.write(f\"Saved predictions to {outname}\")\n",
        "        with open(outname, \"rb\") as f:\n",
        "            st.download_button(\"Download predictions\", f, file_name=outname)\n",
        "'''\n",
        "\n",
        "with open(\"streamlit_app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Wrote streamlit_app.py. To run locally: `streamlit run streamlit_app.py`\")\n",
        "\n",
        "# Done\n",
        "print(\"\\nALL DONE. Generated artifacts:\")\n",
        "print(\" - models/ (saved tuned models)\")\n",
        "print(\" - cv_plots/ (cv R2 plots)\")\n",
        "print(\" - importance_plots/ (SHAP / importance plots)\")\n",
        "print(\" - predictions_full_dataset.xlsx\")\n",
        "print(\" - tuning_summary.csv\")\n",
        "print(\" - streamlit_app.py\")\n"
      ]
    }
  ]
}